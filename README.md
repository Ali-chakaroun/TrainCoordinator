# Train Coordinator ðŸš† (Prototype)

This is a **prototype Train Coordinator service** designed for orchestrating workflows and enforcing access control in **privacy-preserving data analysis** for federated healthcare environments.  

This project extends the **FAIR Data Train (FDT)** framework by integrating:  
- **Common Workflow Language (CWL)** for multi-step workflow execution.  https://www.commonwl.org/user_guide/introduction/quick-start.html
- **Open Digital Rights Language (ODRL)** for fine-grained data access control.  https://www.w3.org/TR/odrl-model/
---

## ðŸ“Œ Features

- **Automated multi-step analysis** â€” chain multiple trains with conditional execution.  
- **Policy-based access control** â€” enforce data retrieval policies usage with ODRL.  
- **Reusable workflows** â€” built using CWL for portability and reproducibility.



| Endpoint                                          | Method | Description                                                                                                                                                         |
| ------------------------------------------------- | ------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `http://localhost:6060/api/analysis/execute`      | `POST` | Executes a CWL workflow and returns the result.                                                                                                                     |
| `http://localhost:6060/api/analysis/odrl-execute` | `POST` | Executes an ODRL policy request and returns the corresponding SPARQL query. *(Note: CWL workflows internally call this endpoint when verifying train permissions.)* |

### `/api/analysis/odrl-execute` Payload Format

This endpoint expects the request body in **raw** format, describing an ODRL request.  
Example payload:

```turtle
@prefix odrl:   <http://www.w3.org/ns/odrl/2/> .
@prefix ex:     <http://example.org/> .

<http://example.org/request:se-query>
    a odrl:Request ;
    odrl:uid <https://www.wikidata.org/wiki/Q25670> ;
    odrl:profile <https://www.wikidata.org/wiki/Q4382010> ;

    odrl:permission [
        odrl:target <http://example.org/graph/extract_data> ;
        odrl:assignee ex:researcher ;
        odrl:action odrl:read
    ] .

```
---

## ðŸ“‚ File Paths

The prototype stores its **workflows** and **local RDF-based data stations** in: **src/main/resources/workflow**

This folder contains:  
- **CWL Workflow files** â€” define the steps, inputs, and conditional execution logic for the analysis.  
- **RDF files** â€” serve as *local data stations* from which the workflow extracts data using SPARQL queries.  

### **ODRL Engine**
The **ODRL engine configuration and policies** used for access control are located in: **src/main/resources/ODRL**

This folder contains:
- **Policies** â€” define permissions, prohibitions, and obligations for data access.  
- **Engine** â€” policy engine that validates access requests against internal policies and if allowed, returns the corresponding SPARQL query file to execute.
- **Data** - holds a sparql query for the requested data when the an data access request is accepted.

### Output Directory
The `output` folder stores the resulting data generated by the workflow execution.

## ðŸ§© Installation & Setup (Prototype)

âš  **Note:**  
This prototype **only runs on Linux** based OS (Debian/Ubuntu)
---

### 1) Prerequisites
- **Java JDK**: 17+
- **Maven**: 3.6+
- **Python**: 3.9+
- **Docker**: latest

### 2) Clone the repository
```bash
git clone to/do
cd TrainCoordinator
```
### 3) Dockerize python scripts
```bash
cd scripts
# build each image
docker build -t extract-adr .           
docker build -t extract-lareb-data .    
docker build -t extract-vigi-data .     
docker build -t extract-sideeff-data . 
```
### 4) install 'cwltool'
```bash
python3 -m pip install --upgrade pip
pip install cwltool
```
### 5) Run the service
```bash
mvn clean package
mvn spring-boot:run
```
